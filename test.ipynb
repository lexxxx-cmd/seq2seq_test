{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零开始线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w,b,num_examples):\n",
    "    '''生成y=Xw+b噪声'''\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    y = torch.matmul(X,w)+b\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size,features,labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i:min(i+batch_size,num_examples)]\n",
    "        )\n",
    "        yield features[batch_indices],labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0,0.01,size=(2,1),requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    return torch.matmul(X,w)+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat,y):\n",
    "    return (y_hat-y.reshape(y_hat.shape))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,lr,batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param-=lr*param.grad/batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.03\n",
    "num_epochs=3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l=loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features,w,b),labels)\n",
    "        print(f'epoch{epoch+1},loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归使用API简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "'''读取数据'''\n",
    "def load_array(data_arrays,batch_size,is_train=True):\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)\n",
    "batch_size = 10\n",
    "data_iter = load_array((features,labels),batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(2,1))\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(net.parameters(),lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        opt.zero_grad()\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch {epoch+1},loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像分类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"../dataset\",\n",
    "    train = True,\n",
    "    transform=trans,download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root=\"../dataset\",\n",
    "    train = False,\n",
    "    transform=trans,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.DataLoader(mnist_train,32,True)\n",
    "test_iter = data.DataLoader(mnist_test,32,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat,y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    if len(y_hat.shape)>1 and y_hat.shape[1]>1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype)==y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def updater(batch_size):\n",
    "    return sgd([W, b], lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch3(net,test_iter,n=6):\n",
    "    for X,y in test_iter:\n",
    "        break\n",
    "    true = y\n",
    "    pred = net(X).argmax(axis=1)\n",
    "    print(true)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ch3(net,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "net.apply(init_weights)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.CrossEntropyLoss(reduction='None')\n",
    "loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(net.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):  #@save\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式\n",
    "    metric = Accumulator(2)  # 正确预测数、预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X.to(device)).detach().cpu(), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):  #@save\n",
    "    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n",
    "    # 将模型设置为训练模式\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # 训练损失总和、训练准确度总和、样本数\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并更新参数\n",
    "        y_hat = net(X.to(device))\n",
    "        l = loss(y_hat, y.to(device))\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用PyTorch内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat.detach().cpu(), y.detach().cpu()), y.detach().cpu().numel())\n",
    "    # 返回训练损失和训练精度\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"训练模型（定义见第3章）\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch3(net,test_iter,n=6):\n",
    "    for X,y in test_iter:\n",
    "        break\n",
    "    true = y\n",
    "    pred = net(X.to(device)).detach().cpu().argmax(axis=1)\n",
    "    print(true)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ch3(net,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机从零开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "bs = 16\n",
    "lr = 0.1\n",
    "epoch = 10\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hiddens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据初始化\n",
    "trans = transforms.ToTensor()\n",
    "train_Data = torchvision.datasets.MNIST(\n",
    "    root = \"../dataset/\",\n",
    "    train = True,\n",
    "    download=True,\n",
    "    transform=trans)\n",
    "test_Data = torchvision.datasets.MNIST(\n",
    "    root = \"../dataset/\",\n",
    "    train = False,\n",
    "    download=True,\n",
    "    transform=trans)\n",
    "train_iter = data.DataLoader(train_Data,bs,True)\n",
    "test_iter = data.DataLoader(test_Data,bs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重初始化\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = nn.Parameter(torch.randn(\n",
    "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(\n",
    "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法\n",
    "    return (H@W2 + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,loss:0.018360688431886957\n",
      "epoch:2,loss:0.02544479181638259\n",
      "epoch:3,loss:0.03029485331397009\n",
      "epoch:4,loss:0.03391813324080673\n",
      "epoch:5,loss:0.03669297842666905\n",
      "epoch:6,loss:0.0389666342414633\n",
      "epoch:7,loss:0.040689516837054907\n",
      "epoch:8,loss:0.04205986548701003\n",
      "epoch:9,loss:0.043110383928773775\n",
      "epoch:10,loss:0.04389703679359182\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "for e in range(epoch):\n",
    "    for X,y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        ls = loss(y_hat,y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        ls.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(ls.mean().detach().numpy())\n",
    "\n",
    "    #计算平均\n",
    "    avg_ls = sum(losses[-len(train_Data):])/len(train_Data)\n",
    "    print(f\"epoch:{e+1},loss:{avg_ls}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for X,y in test_iter:\n",
    "        y_hat = net(X)\n",
    "        res = y_hat.argmax(1)\n",
    "        print(y)\n",
    "        print(res)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "bs = 16\n",
    "lr = 0.1\n",
    "epoch = 10\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hiddens = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据初始化\n",
    "trans = transforms.ToTensor()\n",
    "train_Data = torchvision.datasets.MNIST(\n",
    "    root = \"../dataset/\",\n",
    "    train = True,\n",
    "    download=True,\n",
    "    transform=trans)\n",
    "test_Data = torchvision.datasets.MNIST(\n",
    "    root = \"../dataset/\",\n",
    "    train = False,\n",
    "    download=True,\n",
    "    transform=trans)\n",
    "train_iter = data.DataLoader(train_Data,bs,True)\n",
    "test_iter = data.DataLoader(test_Data,bs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化权重\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(net.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,loss:0.01848777808760448\n",
      "epoch:2,loss:0.025609011399627585\n",
      "epoch:3,loss:0.030505573893068745\n",
      "epoch:4,loss:0.03410685307652699\n",
      "epoch:5,loss:0.03688734450017946\n",
      "epoch:6,loss:0.039101340002926005\n",
      "epoch:7,loss:0.04080757116000398\n",
      "epoch:8,loss:0.042185578079716346\n",
      "epoch:9,loss:0.0432394524592976\n",
      "epoch:10,loss:0.044033875508109534\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "net.to(device)\n",
    "for e in range(epoch):\n",
    "    for X,y in train_iter:\n",
    "        y_hat = net(X.to(device))\n",
    "        ls = loss(y_hat,y.to(device))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        ls.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(ls.mean().detach().cpu().numpy())\n",
    "\n",
    "    #计算平均\n",
    "    avg_ls = sum(losses[-len(train_Data):])/len(train_Data)\n",
    "    print(f\"epoch:{e+1},loss:{avg_ls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for X,y in test_iter:\n",
    "        y_hat = net(X.to(device))\n",
    "        res = y_hat.detach().cpu().argmax(1)\n",
    "        print(y)\n",
    "        print(res)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = torch.optim.SGD([{\"params\":net[0].weight,'weight_decay':3},\n",
    "                               {\"params\":net[0].bias}],lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256,256),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "def init_weights(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kaggle房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#数据预处理\n",
    "#将所有缺失的值替换为相应特征的平均值。\n",
    "#为了将所有特征放在一个共同的尺度上， 我们通过将特征重新缩放到零均值和单位方差来标准化数据\n",
    "# 若无法获得测试数据，则可根据训练数据计算均值和标准差\n",
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理缺失值\n",
    "# “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = train_data.shape[0]\n",
    "#通过values属性，我们可以 从pandas格式中提取NumPy格式，并将其转换为张量表示用于训练\n",
    "train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
    "test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
    "train_labels = torch.tensor(\n",
    "    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "loss = nn.MSELoss()\n",
    "in_features = train_features.shape[1]\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(in_features,1))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(net,features,labels):\n",
    "    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n",
    "    clipped_preds = torch.clamp(net(features),1,float('inf'))\n",
    "    rmse = torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels)))\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_features,train_labels,test_features,test_lables,num_epochs,lr,wd,bs):\n",
    "    train_ls,test_ls = [],[]\n",
    "    train_iter = load_array((train_features,train_labels),bs)\n",
    "    opt = torch.optim.Adam(net.parameters(),lr,weight_decay=wd)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        for X,y in train_iter:\n",
    "            opt.zero_grad()\n",
    "            l = loss(net(X),y)\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "        train_ls.append(log_rmse(net,train_features,train_labels))\n",
    "        if test_lables is not None:\n",
    "            test_ls.append(log_rmse(net,test_features,test_lables))\n",
    "    return train_ls,test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k交叉验证\n",
    "def get_k_fold_data(k,i,x,y):\n",
    "    fold_size=X.shape[0]\n",
    "    X_train,y_train = None,None\n",
    "\n",
    "    for j in range(k):\n",
    "        idx = slice(j*fold_size,(j+1)*fold_size)\n",
    "        X_part,y_part = X[idx,:],y[idx]\n",
    "        if j ==i:\n",
    "            X_valid,y_valid =  X_part,y_part\n",
    "        elif X_train is None:\n",
    "            X_train,y_train = X_part,y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n",
    "           batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n",
    "                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],\n",
    "                     legend=['train', 'valid'], yscale='log')\n",
    "        print(f'折{i + 1}，训练log rmse{float(train_ls[-1]):f}, '\n",
    "              f'验证log rmse{float(valid_ls[-1]):f}')\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
